<!DOCTYPE html>
<html>
  <head>
      <script>
  var _hmt = _hmt || []
  ;(function() {
    var hm = document.createElement('script')
    hm.src = 'https://hm.baidu.com/hm.js?5a0acc897fd96474a2c8f4deac84611a'
    var s = document.getElementsByTagName('script')[0]
    s.parentNode.insertBefore(hm, s)
  })()
</script> 
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="keywords" content="Mino,前端,后端,全栈,NodeJs,JavaScript" />
    <meta name="description" content="Mino&#39;s personal blog" />
    
    <title>
      scrapy爬虫细节 - Mino
    </title>
    <link rel="manifest" href="/manifest.json" />
    <link rel="shortcut icon" href="/images/favicon.ico" type="image/x-icon" />
    <link rel="stylesheet" href="/style/style.css">
  <link rel="alternate" href="/atom.xml" title="Mino" type="application/atom+xml">
</head>
  <body>
    
    <div id="post-toc" class="animated hiddenToc hide">
      <span class="title">Toc</span>
      <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#commit"><span class="toc-text">scrapy.Request相关</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#forin"><span class="toc-text">使用for...in 循环导致数据重复问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#fanye"><span class="toc-text">scrapy翻页</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#CrawlSpider"><span class="toc-text">创建CrawlSpider爬虫简要步骤</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#re"><span class="toc-text">正则的使用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#buchong"><span class="toc-text">CrawlSpider补充知识</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#monidl"><span class="toc-text">模拟登录的原因</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#dir"><span class="toc-text">dir函数的使用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#proxy"><span class="toc-text">添加代理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#sendpost"><span class="toc-text">发送post请求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#zidongpost"><span class="toc-text">自动寻找post的action进行提交</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#contains"><span class="toc-text">xpath的contains语法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#xiayiye"><span class="toc-text">提取下一页的文本</span></a></li></ol>
    </div>
    
    <div id="fixed-menu-wrap">
      <span class="iconfont icon-sousuo search-box menu-reset"></span>
      <span class="icon-toc menu-reset">Toc</span>
      <span class="iconfont icon-arrowup menu-reset"></span>
    </div>
    <div id="fixed-menu">
      <span class="iconfont icon-menu-"></span>
    </div>
    <div id="progress">
      <div class="line"></div>
    </div>
    <div id="search-shade" class="animated hiddenSearch hide">
      <div class="input-wrap">
        <span class="iconfont icon-sousuo search-box"></span>
        <input type="text" placeholder="Search" />
        <span class="iconfont icon-close"></span>
      </div>
      <div class="search-result">
        <div class="meta">
          <span><b id="result-count">0</b> results found</span>
          <img src="/images/logo.jpg" />
        </div>
        <ul id="result-box"></ul>
      </div>
    </div>
    <div id="menu-mask" class="animated hideMenuMask hide">
      <span class="iconfont icon-close"></span>
      <div class="nav">
        
        <a href="/" class="">
          首页
        </a>
        
        <a href="/about" class="">
          关于
        </a>
        
        <a href="/tags" class="">
          标签
        </a>
        
        <a href="/categories" class="">
          分类
        </a>
        
        <a href="/archives" class="">
          归档
        </a>
        
      </div>
    </div>
    <div id="header">
      <div class="intro">
        <a href="/" class="logo" style="background-image: url('/images/logo.jpg')"></a>
        <div class="author">Mino</div>
      </div>
      <div class="nav">
        <span class="iconfont icon-menu menu-icon"></span>
        <a href="#" class="search-box">
          <span class="iconfont icon-sousuo"></span>
        </a>
      </div>
    </div>
    <div id="side" class="animated bounceInLeft">
      <div class="shrink">
        <a href="/" class="logo" style="background-image: url('/images/logo.jpg')"></a>
        <span class="iconfont icon-menu toggle-icon"></span>
        <a href="#" class="search-box">
          <span class="iconfont icon-sousuo"></span>
        </a>
      </div>
      <div class="magnify">
        <div class="about">
          <div class="author">Mino</div>
          <a href="/" class="logo" style="background-image: url('/images/logo.jpg')"></a>
        </div>

        <div class="nav">
          
          <a href="/" class="">
            首页
          </a>
          
          <a href="/about" class="">
            关于
          </a>
          
          <a href="/tags" class="">
            标签
          </a>
          
          <a href="/categories" class="">
            分类
          </a>
          
          <a href="/archives" class="">
            归档
          </a>
          
          <a href="#" class="search-box">
            <span class="iconfont icon-sousuo"></span>
          </a>
        </div>
        <div class="bottom">
          <div class="follow">
            
            <a href="https://github.com/codermino" target="_block">
              <span class="iconfont icon-github"></span>
            </a>
            
            <a href="1227657064@qq.com" target="_block">
              <span class="iconfont icon-QQ"></span>
            </a>
             
            <a href="/atom.xml" target="_block">
              <span class="iconfont icon-rss"></span>
            </a>
            
          </div>
        </div>
      </div>
    </div>
    <div id="container">
      <div class="main animated bounceInRight delay-0.7s">
        <article class="post-entry">
    <div class="header">
      
      <div class="title">scrapy爬虫细节</div>
      <div class="meta">
        <span class="item">
          <span class="iconfont icon-time-circle"></span>
          <span>2020/07/25</span>
        </span>

        

         
        <span class="item">
          <span class="iconfont icon-folder"></span>
          <span>
              
                
                  <a href="/categories/scrapy">scrapy</a>
                
              
          </span>
        </span>
        
        
         
          <span class="item">
            <span class="iconfont icon-tag1"></span>
            <span>
                
                  
                    <a href="/tags/scrapy爬虫细节">scrapy爬虫细节</a>
                  
                
            </span>
          </span>
         
      </div>
      <div>
      </div>
    </div>
    <p><a href="/images/scrapy/scrapy.jpg" data-caption data-fancybox="images"><img src="/images/scrapy/scrapy.jpg" alt></a></p>
<h3 id="commit">scrapy.Request相关<a class="post-anchor" href="#commit"></a><a class="post-anchor" href="#commit"></a></h3>
<pre><code class="hljs python"><span class="hljs-keyword">yield</span> scrapy.Request(
   url,
   callback,
   meta = &#123;<span class="hljs-string">"item"</span>:item&#125;
)
在对应的callback函数中可以使用下面的代码，接收上一个函数传递过来的参数。
item=response.meta[<span class="hljs-string">'key'</span>]</code></pre>

<h3 id="forin">使用for...in 循环导致数据重复问题<a class="post-anchor" href="#forin"></a><a class="post-anchor" href="#forin"></a></h3>
<pre><code class="hljs markdown">在scrapy中使用了for in 循环，由于是多线程，如果还需要继续scrapy.Request进行请求更深的页面数据。
可以使用form copy import deepcopy 进行深拷贝。
可以防止scrapy多线程导致item的数据重复(上一个数据传递过来之后，后面的数据还没有解析完成，
下一个for循环，将传递下去的item的某些字段覆盖，从而可能导致重复)</code></pre>

<h3 id="fanye">scrapy翻页<a class="post-anchor" href="#fanye"></a><a class="post-anchor" href="#fanye"></a></h3>
<pre><code class="hljs python">response.text =&gt; unicode编码返回body部分，等同于 response.body.decode(response.encoding)
page_count = int(re. findall("var pagecount=.7);, response.body.decode())[0])
current_page = int(re. findall"var currentPage=.2);, response.body.decode())[0]
if current_page page_count:
    next_url item["s_href"] +"?pageNumber=[&amp;sort=". format(current_page 1)
    yield scrapy. Request(
        next_url,
        callback=self.parse_book_list,
        meta = &#123;"item":response.meta["item"]&#125;
    )</code></pre>

<h3 id="CrawlSpider">创建CrawlSpider爬虫简要步骤<a class="post-anchor" href="#CrawlSpider"></a><a class="post-anchor" href="#CrawlSpider"></a></h3>
<pre><code class="hljs markdown">1.创建项目文件：
<span class="hljs-code">    scrapy startproject douyu</span>
2.进入项目文件：
<span class="hljs-code">    cd douyu</span>
3.创建爬虫
<span class="hljs-code">    scrapy genspider -t crawl dy 'douyu.com'</span></code></pre>

<h3 id="re">正则的使用<a class="post-anchor" href="#re"></a><a class="post-anchor" href="#re"></a></h3>
<pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">parse_item</span><span class="hljs-params">(self, response)</span>:</span>
    item = &#123;&#125;
    item[<span class="hljs-string">"title"</span>]=re.findall(<span class="hljs-string">"&lt;!---TitleStart--&gt;(.*)&lt;--TitleEnd---&gt;"</span>, response.body. decode())[<span class="hljs-number">0</span>]
    item[<span class="hljs-string">"publish_date"</span>] = re.findall(<span class="hljs-string">"发布时间:(20\d&#123;2&#125;-\d&#123;2&#125;-\d&#123;2&#125;)"</span>, response.body. decode())[<span class="hljs-number">0</span>]</code></pre>

<h3 id="buchong">CrawlSpider补充知识<a class="post-anchor" href="#buchong"></a><a class="post-anchor" href="#buchong"></a></h3>
<pre><code class="hljs markdown">CrawlSpider补充(了解)
 
LinkExtractor更多常见参数:
<span class="hljs-code">    allow:满足括号中“正则表达式的URL会被提取,如果为空,则全部匹配。</span>
<span class="hljs-code">        (就是在当前的爬虫的页面，如果为True就会寻找当前页面是否有满足当前rule的连接，如果为False就不会继续寻找)</span>
<span class="hljs-code">    deny:满足括号中“正则表达式"的URL一定不提取(优先级高于 allow)</span>
<span class="hljs-code">    allow_domains:会被提取的链接的 domains.</span>
<span class="hljs-code">    deny_domains:一定不会被提取链接的 domains</span>
<span class="hljs-code">    restrict_xpaths:使用 xpath表达式和 allow共同作用过滤链接,级 xpath足范围内的ur地址会被提取</span>
<span class="hljs-code">    </span>
spiders.rule常见参数:
<span class="hljs-code">    linkextractor:是一个 Link Extractor对象,用于定义需要提取的链接。</span>
<span class="hljs-code">    calLback:从linkextractor中每获取到链接时,参数所指定的值作为回调函数</span>
<span class="hljs-code">    follow:是一个布尔(boolean)值,指定了根据该规则从 response提取的链接是否需要跟进。</span>
<span class="hljs-code">            如果 callback为none, follow默认设置为Tre,否则默认为 False</span>
<span class="hljs-code">    process_links:指定该 spider中哪个的函数将会被调用,从 linkextractor中获取到链接列表时将会调用该函数,</span>
<span class="hljs-code">                    该方法主要用来过滤url</span>
<span class="hljs-code">    process_request:指定该 spider中哪个的函数将会被调用,该规则提取到每个 request时都会调用该函数,</span>
<span class="hljs-code">                    用来过滤 request</span></code></pre>

<h3 id="monidl">模拟登录的原因<a class="post-anchor" href="#monidl"></a><a class="post-anchor" href="#monidl"></a></h3>
<pre><code class="hljs markdown"> scrapy模拟登陆
为什么需要模拟登陆?
<span class="hljs-code">    获取 cookie,能够爬取登陆后的页面</span>
回顾:
 requests是如何模拟登陆的?
<span class="hljs-code">    1、直接携带 cookies请求页面</span>
<span class="hljs-code">    2、找接口发送post请求 cookie存储</span>
 selenium是如何模拟登陆的?
<span class="hljs-code">    找到对应的 input标签,输入文字点击登录</span></code></pre>

<h3 id="dir">dir函数的使用<a class="post-anchor" href="#dir"></a><a class="post-anchor" href="#dir"></a></h3>
<pre><code class="hljs python">在python2<span class="hljs-number">.7</span>中, dir(response)查看response内置函数可以找到decode, encode()方法。
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">CheckUserAgent</span>:</span>
 <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">process_response</span><span class="hljs-params">(self, request, response, spider)</span>:</span>
     print(dir(response))
     print(request.headers[User-Agent<span class="hljs-string">"])</span></code></pre>

<h3 id="proxy">添加代理<a class="post-anchor" href="#proxy"></a><a class="post-anchor" href="#proxy"></a></h3>
<pre><code class="hljs python">class RandomUserAgent(object):
    def process_request(self, request, spider):
    useragent= random.choice(UER_AGENTS)
    request.headers["User-Agent"] = useragent #添加自定义的UA,给 requestheaders的赋值即可

class ProxyMiddleware(object)
    def process_request(self, request, spider):
        reqeust.meta ["proxy"] = "http://124.115.126.76: 808"
            #添加代理,需要在request的mea信息中添加字段
            #代理的形式为:协议+ip地址+端口</code></pre>

<h3 id="sendpost">发送post请求<a class="post-anchor" href="#sendpost"></a><a class="post-anchor" href="#sendpost"></a></h3>
<pre><code class="hljs python">使用scrapr.Request的时候一直是发送的get请求，
那么post请求就是需要使用scrapy.FormRequest来发送，
同时使用formdata来携带需要post的数据

 scrapy模拟登陆之发送post请求
     class GithubSpider(scrapy.Spider):
        name =github'
        allowed_domains ['github.com']
        start_urls = ['https://github.com/login' -请求首页,为了获取登录参数
         headers =&#123;
        "Accept":"**",
         "Accept-Language": "en-US, en;q=. 8, zh-TW;q=0.6, zh; q=0.4",
         &#125;
        def parse(self, response):
             print(response.url)
             utf8 =  response.xpath("//form[@action='/session']/div[1]/input[1]/@value").extract_first()
             authenticity_token = response.xpath("//form[@action='/session']/div[1]/input[2]/@value").extract_first()
             return scrapy.FormRequest(
                "https://github.com/session",
                headers=self.headers, #可以在spider中定义,也可以在 setting中定义
                formdata = dict(
                    commit="Sign in",
                    utf8=utf8,
                    authenticity_token=authenticity_token,
                     login="user_name",(对应的标签的name)
                     password="password"(对应的标签的name)
                ),
                callbackself =  self.after_login #登录之后的回调函数
             )
             def after_login(self, response):
                print(response.url,"*"*100, response.status)
            注意 : github的部分页面只允许一处登录,比如https:/github.comsettings/security</code></pre>

<h3 id="zidongpost">自动寻找post的action进行提交<a class="post-anchor" href="#zidongpost"></a><a class="post-anchor" href="#zidongpost"></a></h3>
<pre><code class="hljs python">import scrapy
import re

class Github2Spider(scrapy.Spider):
   name =github2'
   allowed_domains ['github.com']
   start_urls  =['https://github.com/login']
   def parse(self, response):
       yield scrapy.FormRequest.from_response(
           response, #自动的从response中寻找from表单
           formdata = &#123;"login":"noobpythoner","password":"zhoudawei1123"&#125;,
           callback=self.after_login
       )
   def after_login(self, response):
       print(re.findall("noobpythoner|NoobPythoner", response.body.decode()))</code></pre>

<h3 id="contains">xpath的contains语法<a class="post-anchor" href="#contains"></a><a class="post-anchor" href="#contains"></a></h3>
<pre><code class="hljs python">div_list = response.xpath(<span class="hljs-string">"//div[contains(@class,'i')])</span></code></pre>

<h3 id="xiayiye">提取下一页的文本<a class="post-anchor" href="#xiayiye"></a><a class="post-anchor" href="#xiayiye"></a></h3>
<pre><code class="hljs python">next_url = response.xpath("//a[text()='下一页']/@href).extract_first()
if next_url is not None:
    next_url = urllib</code></pre>




  
    <div class="post-reward">
    <div id="reward-button">打赏</div>
      <div id="qr">
        <div class="wrap">
            
            <div class="bg-wrap">
              <a href="/images/zhifubao.png" target="_block" class="bg" style="background-image:url('/images/zhifubao.png')"></a>
              支付宝
            </div>
            
            
            <div class="bg-wrap">
                <a href="/images/weixin.png" target="_block" class="bg" style="background-image:url('/images/weixin.png')"></a>
              微信
            </div>
            
        </div>
      </div>
    </div>
  
  <div class="post-guide">
    <div class="item left">
        
    </div>
    <div class="item right">
        
          <a href="/2020/07/25/Excel%E6%89%93%E5%BC%80UTF8%E7%BC%96%E7%A0%81CSV%E6%96%87%E4%BB%B6%E4%B9%B1%E7%A0%81/">Excel打开UTF8编码CSV文件乱码</a>
        
    </div>
  </div>

  

  <div class="post-copyright">
    <div class="auth">
      本文作者：<a href="https://codermino.github.io">Mino</a>
    </div>
    <div class="link">
      永久链接：<a href="https://codermino.github.io/2020/07/25/scrapy爬虫细节/">https://codermino.github.io/2020/07/25/scrapy爬虫细节/</a>
    </div>
    <div class="declare">
      版权声明：本文首发于<a href="https://codermino.github.io">Mino</a>的博客，转载请注明出处！
    </div>
  </div>

  <div id="comment"></div>

  
  
</article>
        <footer>
          <div class="copyright">
            ©2020
            <a href="https://codermino.github.io">Mino</a> Powered by <a href="https://hexo.io" target="_blank" rel="noopener">Mino</a> |
<!--            <a href="https://github.com/shixiaohu2206/hexo-theme-huhu" target="_blank" rel="noopener">hexo-theme-huhu</a>-->
          </div>
          
        </footer>
      </div>
    </div>
  </body>
  
</html>
<script type="text/javascript">
                  window.HUHU_CONFIG = JSON.parse("{\"share\":[\"weibo\",\"weixin\",\"qqkongjian\",\"QQ\",\"douban\",\"facebook\",\"twitter\",\"google\"],\"service_worker\":{\"open\":false}}")
                </script> <script type="text/javascript">window.addEventListener('load', function() {
    
    window.loadJs = function(d, m, a) {
      var c = document.getElementsByTagName('head')[0] || document.head || document.documentElement
      var b = document.createElement('script')
      b.defer = true
      b.setAttribute('type', 'text/javascript')
      b.setAttribute('charset', 'UTF-8')
      b.setAttribute('async', 'true')
      b.setAttribute('src', d)
      m && b.setAttribute('data-main', '/scripts/app-built')
      if (typeof a === 'function') {
        if (window.attachEvent) {
          b.onreadystatechange = function() {
            var e = b.readyState
            if (e === 'loaded' || e === 'complete') {
              b.onreadystatechange = null
              a()
            }
          }
        } else {
          b.onload = a
        }
      }
      c.appendChild(b)
    }
    window.loadJs && window.loadJs('https://cdn.bootcss.com/require.js/2.3.6/require.min.js', true, function() {require.config({"paths":{"util":"util","share":"share","search":"search","registerSW":"registerSW","valine":"cdn/Valine.min","av":["https://cdn.jsdelivr.net/npm/leancloud-storage/dist/av-min"],"pjax":["https://cdn.bootcss.com/jquery.pjax/2.0.1/jquery.pjax.min"],"jquery":["https://cdn.bootcss.com/jquery/3.4.1/jquery.min"],"confirm":["https://cdn.bootcss.com/jquery-confirm/3.3.4/jquery-confirm.min"],"fancybox":["https://cdn.bootcss.com/fancybox/3.5.7/jquery.fancybox.min"],"chart":["https://cdn.bootcss.com/Chart.js/2.8.0-rc.1/Chart.bundle.min"]},"map":{"*":{"css":"https://cdn.bootcss.com/require-css/0.1.10/css.min.js"}},"shim":{"fancybox":{"deps":["css!https://cdn.bootcss.com/fancybox/3.5.7/jquery.fancybox.min.css"]},"confirm":{"deps":["css!https://cdn.bootcss.com/jquery-confirm/3.3.4/jquery-confirm.min.css"]},"chart":{"deps":["css!https://cdn.bootcss.com/Chart.js/2.8.0-rc.1/Chart.min.css"]}},"waitSeconds":3})})
  })</script> <script type="text/javascript">
                  ;(function() {
                    var bp = document.createElement('script')
                    var curProtocol = window.location.protocol.split(':')[0]
                    if (curProtocol === 'https') {
                      bp.src = 'https://zz.bdstatic.com/linksubmit/push.js'
                    } else {
                      bp.src = 'http://push.zhanzhang.baidu.com/push.js'
                    }
                    var s = document.getElementsByTagName('script')[0]
                    s.parentNode.insertBefore(bp, s)
                  })()
                </script> 
